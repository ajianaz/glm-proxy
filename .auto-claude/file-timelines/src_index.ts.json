{
  "file_path": "src/index.ts",
  "main_branch_history": [],
  "task_views": {
    "006-implement-in-memory-api-key-cache-with-ttl-to-elim": {
      "task_id": "006-implement-in-memory-api-key-cache-with-ttl-to-elim",
      "branch_point": {
        "commit_hash": "e4ccb2c239067a08687940247e7dc3c37228e546",
        "content": "import { Hono } from 'hono';\nimport { cors } from 'hono/cors';\nimport { getModelForKey } from './validator.js';\nimport { proxyRequest } from './proxy.js';\nimport { proxyAnthropicRequest } from './anthropic.js';\nimport { checkRateLimit } from './ratelimit.js';\nimport { authMiddleware, getApiKeyFromContext, type AuthContext } from './middleware/auth.js';\nimport { rateLimitMiddleware } from './middleware/rateLimit.js';\nimport { createProxyHandler } from './handlers/proxyHandler.js';\nimport type { StatsResponse } from './types.js';\n\ntype Bindings = {\n  ZAI_API_KEY: string;\n  DEFAULT_MODEL: string;\n  PORT: string;\n};\n\nconst app = new Hono<{ Bindings: Bindings; Variables: AuthContext }>();\n\n// Enable CORS\napp.use('/*', cors({\n  origin: '*',\n  allowMethods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],\n  allowHeaders: ['Content-Type', 'Authorization', 'x-api-key'],\n}));\n\n// Stats endpoint\napp.get('/stats', authMiddleware, async (c) => {\n  const apiKey = getApiKeyFromContext(c as any);\n\n  // Get rate limit info\n  const rateLimit = checkRateLimit(apiKey);\n\n  // Calculate model\n  const model = getModelForKey(apiKey);\n\n  const stats: StatsResponse = {\n    key: apiKey.key,\n    name: apiKey.name,\n    model,\n    token_limit_per_5h: apiKey.token_limit_per_5h,\n    expiry_date: apiKey.expiry_date,\n    created_at: apiKey.created_at,\n    last_used: apiKey.last_used,\n    is_expired: new Date(apiKey.expiry_date) < new Date(),\n    current_usage: {\n      tokens_used_in_current_window: rateLimit.tokensUsed,\n      window_started_at: rateLimit.windowStart,\n      window_ends_at: rateLimit.windowEnd,\n      remaining_tokens: Math.max(0, rateLimit.tokensLimit - rateLimit.tokensUsed),\n    },\n    total_lifetime_tokens: apiKey.total_lifetime_tokens,\n  };\n\n  return c.json(stats);\n});\n\n// Create proxy handlers\nconst openaiProxyHandler = createProxyHandler(proxyRequest);\nconst anthropicProxyHandler = createProxyHandler(proxyAnthropicRequest);\n\n// Anthropic Messages API - must be defined before /v1/* catch-all\napp.post('/v1/messages', authMiddleware, rateLimitMiddleware, anthropicProxyHandler);\n\n// OpenAI-Compatible API - catch-all for /v1/*\napp.all('/v1/*', authMiddleware, rateLimitMiddleware, openaiProxyHandler);\n\n// Health check\napp.get('/health', (c) => {\n  return c.json({ status: 'ok', timestamp: new Date().toISOString() });\n});\n\n// Root\napp.get('/', (c) => {\n  return c.json({\n    name: 'Proxy Gateway',\n    version: '1.0.0',\n    endpoints: {\n      health: 'GET /health',\n      stats: 'GET /stats',\n      openai_compatible: 'ALL /v1/* (except /v1/messages)',\n      anthropic_compatible: 'POST /v1/messages',\n    },\n  });\n});\n\nconst port = parseInt(process.env.PORT || '3000');\n\nexport default {\n  port,\n  fetch: app.fetch,\n};\n\nconsole.log(`Proxy Gateway starting on port ${port}`);\n",
        "timestamp": "2026-01-22T13:20:55.122162"
      },
      "worktree_state": {
        "content": "import { Hono } from 'hono';\nimport { cors } from 'hono/cors';\nimport { getModelForKey } from './validator.js';\nimport { proxyRequest } from './proxy.js';\nimport { proxyAnthropicRequest } from './anthropic.js';\nimport { checkRateLimit } from './ratelimit.js';\nimport { authMiddleware, getApiKeyFromContext, type AuthContext } from './middleware/auth.js';\nimport { rateLimitMiddleware } from './middleware/rateLimit.js';\nimport { createProxyHandler } from './handlers/proxyHandler.js';\nimport type { StatsResponse, CacheStatsResponse } from './types.js';\nimport { warmupCache } from './storage.js';\nimport { apiKeyCache } from './cache.js';\n\ntype Bindings = {\n  ZAI_API_KEY: string;\n  DEFAULT_MODEL: string;\n  PORT: string;\n};\n\nconst app = new Hono<{ Bindings: Bindings; Variables: AuthContext }>();\n\n// Enable CORS\napp.use('/*', cors({\n  origin: '*',\n  allowMethods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],\n  allowHeaders: ['Content-Type', 'Authorization', 'x-api-key'],\n}));\n\n// Stats endpoint\napp.get('/stats', authMiddleware, async (c) => {\n  const apiKey = getApiKeyFromContext(c as any);\n\n  // Get rate limit info\n  const rateLimit = checkRateLimit(apiKey);\n\n  // Calculate model\n  const model = getModelForKey(apiKey);\n\n  const stats: StatsResponse = {\n    key: apiKey.key,\n    name: apiKey.name,\n    model,\n    token_limit_per_5h: apiKey.token_limit_per_5h,\n    expiry_date: apiKey.expiry_date,\n    created_at: apiKey.created_at,\n    last_used: apiKey.last_used,\n    is_expired: new Date(apiKey.expiry_date) < new Date(),\n    current_usage: {\n      tokens_used_in_current_window: rateLimit.tokensUsed,\n      window_started_at: rateLimit.windowStart,\n      window_ends_at: rateLimit.windowEnd,\n      remaining_tokens: Math.max(0, rateLimit.tokensLimit - rateLimit.tokensUsed),\n    },\n    total_lifetime_tokens: apiKey.total_lifetime_tokens,\n  };\n\n  return c.json(stats);\n});\n\n// Cache statistics endpoint\napp.get('/cache-stats', authMiddleware, async (c) => {\n  const cacheStats = apiKeyCache.getStats();\n  const cacheEnabled = process.env.CACHE_ENABLED !== 'false';\n\n  const stats: CacheStatsResponse = {\n    ...cacheStats,\n    enabled: cacheEnabled,\n  };\n\n  return c.json(stats);\n});\n\n// Create proxy handlers\nconst openaiProxyHandler = createProxyHandler(proxyRequest);\nconst anthropicProxyHandler = createProxyHandler(proxyAnthropicRequest);\n\n// Anthropic Messages API - must be defined before /v1/* catch-all\napp.post('/v1/messages', authMiddleware, rateLimitMiddleware, anthropicProxyHandler);\n\n// OpenAI-Compatible API - catch-all for /v1/*\napp.all('/v1/*', authMiddleware, rateLimitMiddleware, openaiProxyHandler);\n\n// Health check\napp.get('/health', (c) => {\n  return c.json({ status: 'ok', timestamp: new Date().toISOString() });\n});\n\n// Root\napp.get('/', (c) => {\n  return c.json({\n    name: 'Proxy Gateway',\n    version: '1.0.0',\n    endpoints: {\n      health: 'GET /health',\n      stats: 'GET /stats',\n      cache_stats: 'GET /cache-stats',\n      openai_compatible: 'ALL /v1/* (except /v1/messages)',\n      anthropic_compatible: 'POST /v1/messages',\n    },\n  });\n});\n\nconst port = parseInt(process.env.PORT || '3000');\n\nexport default {\n  port,\n  fetch: app.fetch,\n};\n\nconsole.log(`Proxy Gateway starting on port ${port}`);\n\n// Optional cache warm-up on startup (non-blocking)\nif (process.env.CACHE_WARMUP_ON_START === 'true') {\n  // Fire and forget - don't await, let it run in background\n  warmupCache().catch(error => {\n    console.error('Cache warm-up error:', error);\n  });\n}\n",
        "last_modified": "2026-01-22T13:20:55.578532"
      },
      "task_intent": {
        "title": "Implement in-memory API key cache with TTL to eliminate file I/O on every request",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-22T12:46:07.637495",
  "last_updated": "2026-01-22T13:20:55.437862"
}