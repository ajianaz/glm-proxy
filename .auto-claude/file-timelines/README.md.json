{
  "file_path": "README.md",
  "main_branch_history": [],
  "task_views": {
    "006-implement-in-memory-api-key-cache-with-ttl-to-elim": {
      "task_id": "006-implement-in-memory-api-key-cache-with-ttl-to-elim",
      "branch_point": {
        "commit_hash": "e4ccb2c239067a08687940247e7dc3c37228e546",
        "content": "# GLM Proxy\n\nAn API Gateway with rate limiting that proxies requests to Z.AI API (glm-4.7). Supports streaming, REST API, and multi-user token-based quota management.\n\nCreated by [ajianaz](https://github.com/ajianaz)\n\n## Features\n\n- **OpenAI-Compatible**: Proxy endpoint `/v1/*` to Z.AI API\n- **Anthropic-Compatible**: Proxy endpoint `/v1/messages` to Z.AI Anthropic API\n- **Streaming Support**: Full support for Server-Sent Events (SSE)\n- **Rate Limiting**: Token-based quota with rolling 5-hour window\n- **Multi-User**: Multiple API keys with per-key limits\n- **Usage Tracking**: Monitor token usage per key\n- **Model Override**: Set specific model per API key\n\n## Quick Setup\n\n### 1. Environment Configuration\n\n```bash\n# Copy example env file\ncp .env.example .env\n\n# Edit .env\nZAI_API_KEY=your_zai_api_key_here    # Required: Master API key from Z.AI\nDEFAULT_MODEL=glm-4.7                 # Optional: Default model (fallback)\nPORT=3030                             # Optional: Service port\n```\n\n### 2. Start Service\n\n**Docker (Recommended):**\n```bash\ndocker-compose up -d\n```\n\n**Local with Bun:**\n```bash\nbun install\nbun start\n```\n\n## API Documentation\n\n### Endpoints\n\n| Method | Endpoint | Description | Auth Required |\n|--------|----------|-------------|---------------|\n| GET | `/health` | Health check | No |\n| GET | `/stats` | Usage statistics | Yes |\n| POST | `/v1/chat/completions` | Chat completion (OpenAI-compatible) | Yes |\n| POST | `/v1/messages` | Messages API (Anthropic-compatible) | Yes |\n| GET | `/v1/models` | List available models | Yes |\n\n### Authentication\n\nUse API key via header:\n```bash\nAuthorization: Bearer pk_your_api_key\n```\n\nor query parameter:\n```bash\n?api_key=pk_your_api_key\n```\n\n---\n\n## Usage\n\n### 1. Check Health\n\n```bash\ncurl http://localhost:3030/health\n```\n\nResponse:\n```json\n{\n  \"status\": \"ok\",\n  \"timestamp\": \"2026-01-18T00:00:00.000Z\"\n}\n```\n\n### 2. Check Usage/Quota\n\n```bash\ncurl -H \"Authorization: Bearer pk_your_key\" http://localhost:3030/stats\n```\n\nResponse:\n```json\n{\n  \"key\": \"pk_test_key\",\n  \"name\": \"Test User\",\n  \"model\": \"glm-4.7\",\n  \"token_limit_per_5h\": 100000,\n  \"expiry_date\": \"2026-12-31T23:59:59Z\",\n  \"created_at\": \"2026-01-18T00:00:00Z\",\n  \"last_used\": \"2026-01-18T01:00:00.000Z\",\n  \"is_expired\": false,\n  \"current_usage\": {\n    \"tokens_used_in_current_window\": 150,\n    \"window_started_at\": \"2026-01-18T00:00:00.000Z\",\n    \"window_ends_at\": \"2026-01-18T05:00:00.000Z\",\n    \"remaining_tokens\": 99850\n  },\n  \"total_lifetime_tokens\": 150\n}\n```\n\n### 3. Chat Completion (OpenAI-Compatible, Non-Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000\n  }'\n```\n\n### 4. Chat Completion (OpenAI-Compatible, Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n    \"stream\": true\n  }'\n```\n\nStreaming response format (SSE):\n```\ndata: {\"id\":\"...\",\"created\":1234567890,\"object\":\"chat.completion.chunk\",\"model\":\"glm-4.7\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"id\":\"...\",\"created\":1234567890,\"object\":\"chat.completion.chunk\",\"model\":\"glm-4.7\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"}}]}\n\ndata: [DONE]\n```\n\n### 5. Anthropic Messages API (Non-Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"max_tokens\": 1024,\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n  }'\n```\n\n### 6. Anthropic Messages API (Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"max_tokens\": 1024,\n    \"stream\": true,\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n    ]\n  }'\n```\n\n### Using Anthropic SDK (TypeScript/JavaScript)\n\n```typescript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst anthropic = new Anthropic({\n  apiKey: 'pk_your_key',  // Use API key from proxy\n  baseURL: 'http://localhost:3030',  // Proxy base URL (without /v1/messages)\n});\n\nconst msg = await anthropic.messages.create({\n  model: 'glm-4.7',\n  max_tokens: 1024,\n  messages: [{ role: 'user', content: 'Hello, GLM Proxy!' }],\n});\n\nconsole.log(msg.content);\n```\n\n### Using Anthropic SDK (Python)\n\n```python\nimport anthropic\n\nclient = anthropic.Anthropic(\n    api_key='pk_your_key',  # Use API key from proxy\n    base_url='http://localhost:3030',  # Proxy base URL\n)\n\nmessage = client.messages.create(\n    model='glm-4.7',\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, GLM Proxy!\"}\n    ]\n)\n\nprint(message.content)\n```\n\n---\n\n## API Key Management\n\nAPI keys are stored in `data/apikeys.json`. Edit manually to add/remove/modify keys.\n\n### API Key Structure\n\n```json\n{\n  \"keys\": [\n    {\n      \"key\": \"pk_user_12345\",\n      \"name\": \"User Full Name\",\n      \"model\": \"glm-4.7\",\n      \"token_limit_per_5h\": 100000,\n      \"expiry_date\": \"2026-12-31T23:59:59Z\",\n      \"created_at\": \"2026-01-18T00:00:00Z\",\n      \"last_used\": \"2026-01-18T00:00:00Z\",\n      \"total_lifetime_tokens\": 0,\n      \"usage_windows\": []\n    }\n  ]\n}\n```\n\n### Field Configuration\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `key` | string | Unique API key identifier (format: `pk_*`) |\n| `name` | string | User/owner name |\n| `model` | string | Model for this key (glm-4.7, glm-4.5-air, etc.) |\n| `token_limit_per_5h` | number | Token quota per 5-hour rolling window |\n| `expiry_date` | string | ISO 8601 timestamp for expiry |\n| `created_at` | string | ISO 8601 creation timestamp |\n| `last_used` | string | ISO 8601 last usage timestamp (auto-updated) |\n| `total_lifetime_tokens` | number | Total all tokens ever used |\n| `usage_windows` | array | Internal tracking array (auto-managed) |\n\n### Example: Create New API Key\n\n```bash\n# Edit file\nnano data/apikeys.json\n\n# Or with jq\njq '.keys += [{\n  \"key\": \"pk_new_user_'\"$(date +%s)\"'\",\n  \"name\": \"New User\",\n  \"model\": \"glm-4.7\",\n  \"token_limit_per_5h\": 50000,\n  \"expiry_date\": \"2026-12-31T23:59:59Z\",\n  \"created_at\": \"'\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"'\",\n  \"last_used\": \"2026-01-18T00:00:00Z\",\n  \"total_lifetime_tokens\": 0,\n  \"usage_windows\": []\n}]' data/apikeys.json > tmp.json && mv tmp.json data/apikeys.json\n```\n\n---\n\n## Rate Limiting\n\n### Rolling 5-Hour Window\n\n- **Window Type**: Rolling window (not fixed reset)\n- **Duration**: 5 hours\n- **Metric**: Total tokens from all requests within active window\n\n### Calculation Example\n\nIf `token_limit_per_5h = 100,000`:\n\n| Time | Tokens | Active Windows (5h) | Total Used | Status |\n|------|--------|---------------------|------------|--------|\n| 00:00 | 10,000 | [(00:00-05:00, 10K)] | 10,000 | OK |\n| 02:00 | 20,000 | [(00:00-05:00, 10K), (02:00-07:00, 20K)] | 30,000 | OK |\n| 04:00 | 50,000 | [(00:00-05:00, 10K), (02:00-07:00, 20K), (04:00-09:00, 50K)] | 80,000 | OK |\n| 04:30 | 30,000 | [(00:00-05:00, 10K), (02:00-07:00, 20K), (04:00-09:00, 50K), (04:30-09:30, 30K)] | 110,000 | **RATE LIMITED** |\n\n### Rate Limited Response\n\n```json\n{\n  \"error\": \"Rate limit exceeded. Please try again later.\"\n}\n```\n\nHTTP Status: `429 Too Many Requests`\n\n---\n\n## Capacity & Scaling\n\n### Single Instance Capacity\n\nWith default setup (Docker, 1 CPU, 512MB RAM):\n- **Concurrent Requests**: ~50-100\n- **Requests/second**: ~100-500 (depending on response size)\n- **Throughput**: Limited by Z.AI rate limit\n\n### Bottlenecks\n\n1. **Z.AI Rate Limit**: Check Z.AI documentation for limits per API key\n2. **Network**: Bandwidth server <-> Z.AI\n3. **CPU/JSON parsing**: For high-throughput scenarios\n\n### Scaling Options\n\n**Horizontal Scaling (Recommended):**\n```bash\n# Multiple instances behind load balancer\ndocker-compose up --scale proxy-gateway=3\n```\n\n**Vertical Scaling:**\n- Increase CPU/RAM in docker-compose.yml\n- Add Redis for distributed rate limiting\n\n---\n\n## Error Codes\n\n| HTTP Code | Error Type | Description |\n|-----------|------------|-------------|\n| 200 | Success | Request successful |\n| 400 | Bad Request | Invalid request body/params |\n| 401 | Unauthorized | Missing/invalid API key |\n| 403 | Forbidden | API key expired |\n| 429 | Rate Limited | Quota exceeded |\n| 500 | Server Error | Internal server error |\n| 502 | Bad Gateway | Upstream (Z.AI) error |\n\n---\n\n## User Information\n\n### Share with Users\n\nProvide the following information to each user:\n\n```\n\ud83d\udcdd API Access Information\n\nEndpoint: http://your-domain.com/v1/chat/completions\nMethod: POST\nHeaders:\n  Authorization: Bearer YOUR_API_KEY\n  Content-Type: application/json\n\nYour API Key: pk_xxxxx\nQuota: 100,000 tokens per 5 hours\nExpiry: 2026-12-31\n\nCheck quota: http://your-domain.com/stats\nDocumentation: http://your-domain.com/docs\n\nExample Request:\ncurl -X POST http://your-domain.com/v1/chat/completions \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"glm-4.7\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}'\n```\n\n### FAQ\n\n**Q: Does it support streaming?**\nA: Yes, set `\"stream\": true` in the request body for streaming response (both OpenAI and Anthropic formats).\n\n**Q: Does it support Anthropic Messages API?**\nA: Yes! Use endpoint `/v1/messages` with Anthropic format. The proxy will auto-forward to Z.AI Anthropic-compatible API.\n\n**Q: Does it support models other than glm-4.7?**\nA: Yes, glm-4.5-air, glm-4.7, glm-4.5-flash, etc. Check Z.AI docs for full list.\n\n**Q: What if quota runs out?**\nA: Wait until the 5-hour window ends, or request admin to increase limit.\n\n**Q: Is my data stored?**\nA: No logging of request/response. Only token usage is tracked.\n\n**Q: What's the difference between OpenAI-compatible vs Anthropic-compatible?**\nA: OpenAI-compatible (`/v1/chat/completions`) uses OpenAI format. Anthropic-compatible (`/v1/messages`) uses Anthropic Messages API format. Both are proxied to Z.AI glm-4.7.\n\n---\n\n## Troubleshooting\n\n### Container won't start\n```bash\n# Check logs\ndocker-compose logs -f\n\n# Rebuild\ndocker-compose up --build -d\n```\n\n### Port conflict\n```bash\n# Change PORT in .env\nPORT=3031\n\n# Or kill process using port\nlsof -ti:3030 | xargs kill -9\n```\n\n### API key invalid/expired\n```bash\n# Check apikeys.json\ncat data/apikeys.json | jq .\n\n# Update expiry date\njq '.keys[0].expiry_date = \"2027-12-31T23:59:59Z\"' data/apikeys.json > tmp.json && mv tmp.json data/apikeys.json\n```\n\n### Z.AI error\n```bash\n# Check Z.AI_API_KEY is valid\ncurl -H \"Authorization: Bearer YOUR_ZAI_KEY\" https://api.z.ai/api/coding/paas/v4/models\n\n# Check Z.AI rate limit\n# (Need to check in Z.AI dashboard)\n```\n\n---\n\n## Development\n\n### Run tests\n```bash\nbun test\n```\n\n### Build\n```bash\nbun build src/index.ts --outdir /tmp/build\n```\n\n### Type check\n```bash\nbun run typecheck\n```\n\n### Lint\n```bash\nbun run lint\n```\n\n---\n\n## Available Models\n\n| Model | Description | Context | Max Output |\n|-------|-------------|---------|------------|\n| glm-4.7 | High-intelligence flagship | 200K | 96K |\n| glm-4.5-air | High cost-performance | 128K | 96K |\n| glm-4.5-flash | Free model | 128K | 96K |\n\n---\n\n## License\n\nMIT\n\n---\n\n## Support\n\nFor issues and questions, please contact [ajianaz](https://github.com/ajianaz).\n",
        "timestamp": "2026-01-22T13:20:55.122162"
      },
      "worktree_state": {
        "content": "# GLM Proxy\n\nAn API Gateway with rate limiting that proxies requests to Z.AI API (glm-4.7). Supports streaming, REST API, and multi-user token-based quota management.\n\nCreated by [ajianaz](https://github.com/ajianaz)\n\n## Features\n\n- **OpenAI-Compatible**: Proxy endpoint `/v1/*` to Z.AI API\n- **Anthropic-Compatible**: Proxy endpoint `/v1/messages` to Z.AI Anthropic API\n- **Streaming Support**: Full support for Server-Sent Events (SSE)\n- **Rate Limiting**: Token-based quota with rolling 5-hour window\n- **Multi-User**: Multiple API keys with per-key limits\n- **Usage Tracking**: Monitor token usage per key\n- **Model Override**: Set specific model per API key\n- **In-Memory Caching**: LRU cache with TTL for API key lookups (eliminates 95%+ file I/O)\n\n## Quick Setup\n\n### 1. Environment Configuration\n\n```bash\n# Copy example env file\ncp .env.example .env\n\n# Edit .env\nZAI_API_KEY=your_zai_api_key_here    # Required: Master API key from Z.AI\nDEFAULT_MODEL=glm-4.7                 # Optional: Default model (fallback)\nPORT=3030                             # Optional: Service port\n\n# Cache Configuration (Optional)\nCACHE_ENABLED=true                    # Enable/disable in-memory cache (default: true)\nCACHE_TTL_MS=300000                   # Cache TTL in milliseconds (default: 300000 = 5 minutes)\nCACHE_MAX_SIZE=1000                   # Maximum cache entries (default: 1000)\nCACHE_WARMUP_ON_START=false           # Pre-load all keys on startup (default: false)\nCACHE_LOG_LEVEL=none                  # Cache logging: none, info, or debug (default: none)\n```\n\n### 2. Start Service\n\n**Docker (Recommended):**\n```bash\ndocker-compose up -d\n```\n\n**Local with Bun:**\n```bash\nbun install\nbun start\n```\n\n## API Documentation\n\n### Endpoints\n\n| Method | Endpoint | Description | Auth Required |\n|--------|----------|-------------|---------------|\n| GET | `/health` | Health check | No |\n| GET | `/stats` | Usage statistics | Yes |\n| GET | `/cache-stats` | Cache statistics | Yes |\n| POST | `/v1/chat/completions` | Chat completion (OpenAI-compatible) | Yes |\n| POST | `/v1/messages` | Messages API (Anthropic-compatible) | Yes |\n| GET | `/v1/models` | List available models | Yes |\n\n### Authentication\n\nUse API key via header:\n```bash\nAuthorization: Bearer pk_your_api_key\n```\n\nor query parameter:\n```bash\n?api_key=pk_your_api_key\n```\n\n---\n\n## Usage\n\n### 1. Check Health\n\n```bash\ncurl http://localhost:3030/health\n```\n\nResponse:\n```json\n{\n  \"status\": \"ok\",\n  \"timestamp\": \"2026-01-18T00:00:00.000Z\"\n}\n```\n\n### 2. Check Usage/Quota\n\n```bash\ncurl -H \"Authorization: Bearer pk_your_key\" http://localhost:3030/stats\n```\n\nResponse:\n```json\n{\n  \"key\": \"pk_test_key\",\n  \"name\": \"Test User\",\n  \"model\": \"glm-4.7\",\n  \"token_limit_per_5h\": 100000,\n  \"expiry_date\": \"2026-12-31T23:59:59Z\",\n  \"created_at\": \"2026-01-18T00:00:00Z\",\n  \"last_used\": \"2026-01-18T01:00:00.000Z\",\n  \"is_expired\": false,\n  \"current_usage\": {\n    \"tokens_used_in_current_window\": 150,\n    \"window_started_at\": \"2026-01-18T00:00:00.000Z\",\n    \"window_ends_at\": \"2026-01-18T05:00:00.000Z\",\n    \"remaining_tokens\": 99850\n  },\n  \"total_lifetime_tokens\": 150\n}\n```\n\n### 3. Chat Completion (OpenAI-Compatible, Non-Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000\n  }'\n```\n\n### 4. Chat Completion (OpenAI-Compatible, Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n    \"stream\": true\n  }'\n```\n\nStreaming response format (SSE):\n```\ndata: {\"id\":\"...\",\"created\":1234567890,\"object\":\"chat.completion.chunk\",\"model\":\"glm-4.7\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"id\":\"...\",\"created\":1234567890,\"object\":\"chat.completion.chunk\",\"model\":\"glm-4.7\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"}}]}\n\ndata: [DONE]\n```\n\n### 5. Anthropic Messages API (Non-Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"max_tokens\": 1024,\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n  }'\n```\n\n### 6. Anthropic Messages API (Streaming)\n\n```bash\ncurl -X POST http://localhost:3030/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer pk_your_key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"glm-4.7\",\n    \"max_tokens\": 1024,\n    \"stream\": true,\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n    ]\n  }'\n```\n\n### Using Anthropic SDK (TypeScript/JavaScript)\n\n```typescript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst anthropic = new Anthropic({\n  apiKey: 'pk_your_key',  // Use API key from proxy\n  baseURL: 'http://localhost:3030',  // Proxy base URL (without /v1/messages)\n});\n\nconst msg = await anthropic.messages.create({\n  model: 'glm-4.7',\n  max_tokens: 1024,\n  messages: [{ role: 'user', content: 'Hello, GLM Proxy!' }],\n});\n\nconsole.log(msg.content);\n```\n\n### Using Anthropic SDK (Python)\n\n```python\nimport anthropic\n\nclient = anthropic.Anthropic(\n    api_key='pk_your_key',  # Use API key from proxy\n    base_url='http://localhost:3030',  # Proxy base URL\n)\n\nmessage = client.messages.create(\n    model='glm-4.7',\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, GLM Proxy!\"}\n    ]\n)\n\nprint(message.content)\n```\n\n---\n\n## Cache Architecture\n\n### Overview\n\nThe proxy implements an **in-memory LRU (Least Recently Used) cache** to dramatically reduce file I/O overhead. Every API request requires an authentication check that looks up the API key from `data/apikeys.json`. Without caching, each request triggers a disk read with file locking, creating a bottleneck under load.\n\n### How It Works\n\n```\nRequest \u2192 Auth Middleware \u2192 findApiKey()\n                              \u2193\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Cache Enabled? \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Check Cache     \u2502\u2500\u2500\u2500 Hit? \u2192 Return Cached API Key (<1ms)\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193 Miss\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Read from File  \u2502\u2500\u2500\u2500 Populate Cache \u2192 Return API Key (5-50ms)\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Cache Features\n\n- **TTL Expiration**: Entries expire after 5 minutes (configurable via `CACHE_TTL_MS`)\n- **LRU Eviction**: When cache is full, least recently used entries are evicted\n- **Negative Caching**: Non-existent keys are cached as `null` to prevent repeated lookups\n- **Automatic Updates**: Cache is updated when API key usage is recorded (e.g., token counts)\n- **Optional Warm-up**: Pre-load all keys on startup to eliminate cold starts\n\n### Performance Benefits\n\n| Metric | Without Cache | With Cache | Improvement |\n|--------|---------------|------------|-------------|\n| API key lookup latency | 5-50ms | <1ms | **>10x faster** |\n| File I/O operations | 1 per request | ~0.05 per request | **95% reduction** |\n| Concurrent request capacity | Limited by file locking | 100+ requests | **No contention** |\n\n### Configuration Options\n\n| Environment Variable | Default | Description |\n|---------------------|---------|-------------|\n| `CACHE_ENABLED` | `true` | Enable or disable the cache entirely |\n| `CACHE_TTL_MS` | `300000` | Time-to-live in milliseconds (300000 = 5 minutes) |\n| `CACHE_MAX_SIZE` | `1000` | Maximum number of API keys to cache |\n| `CACHE_WARMUP_ON_START` | `false` | Pre-load all API keys on application startup |\n| `CACHE_LOG_LEVEL` | `none` | Logging verbosity: `none`, `info`, or `debug` |\n\n### Cache Monitoring\n\nCheck cache performance and statistics:\n\n```bash\ncurl -H \"Authorization: Bearer pk_your_key\" http://localhost:3030/cache-stats\n```\n\nResponse:\n```json\n{\n  \"hits\": 1523,\n  \"misses\": 12,\n  \"hitRate\": 99.22,\n  \"size\": 45,\n  \"maxSize\": 1000,\n  \"enabled\": true\n}\n```\n\n**Metrics Explained:**\n- `hits`: Number of successful cache retrievals\n- `misses`: Number of cache misses (required file read)\n- `hitRate`: Percentage of requests served from cache (target: >95%)\n- `size`: Current number of entries in cache\n- `maxSize`: Maximum cache capacity\n- `enabled`: Whether cache is currently enabled\n\n### Cache Coherency\n\nThe cache maintains data consistency through:\n\n1. **TTL Expiration**: Entries auto-expire after 5 minutes, ensuring fresh data\n2. **Write-Through Updates**: When token usage is recorded, the cache is immediately updated\n3. **Selective Invalidation**: Only the affected key is updated, not the entire cache\n4. **Fail-Safe Design**: If the cache is disabled, all operations fall back to file-based storage\n\n### Logging\n\nDebug cache operations by setting `CACHE_LOG_LEVEL`:\n\n```bash\n# Enable debug logging (shows every cache hit/miss)\nCACHE_LOG_LEVEL=debug\n\n# Enable info logging (shows cache updates and warm-up)\nCACHE_LOG_LEVEL=info\n\n# Disable cache logging (default)\nCACHE_LOG_LEVEL=none\n```\n\nExample log output:\n```\n[cache] Cache hit {\"key\":\"pk_user_1...\",\"found\":true}\n[cache] Cache miss - fallback to file {\"key\":\"pk_user_2...\"}\n[cache] Cache populated after file read {\"key\":\"pk_user_2...\",\"found\":true}\n[cache] Cache updated after usage update {\"key\":\"pk_user_1...\",\"tokensUsed\":150,\"totalTokens\":5000}\n```\n\n---\n\nAPI keys are stored in `data/apikeys.json`. Edit manually to add/remove/modify keys.\n\n### API Key Structure\n\n```json\n{\n  \"keys\": [\n    {\n      \"key\": \"pk_user_12345\",\n      \"name\": \"User Full Name\",\n      \"model\": \"glm-4.7\",\n      \"token_limit_per_5h\": 100000,\n      \"expiry_date\": \"2026-12-31T23:59:59Z\",\n      \"created_at\": \"2026-01-18T00:00:00Z\",\n      \"last_used\": \"2026-01-18T00:00:00Z\",\n      \"total_lifetime_tokens\": 0,\n      \"usage_windows\": []\n    }\n  ]\n}\n```\n\n### Field Configuration\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `key` | string | Unique API key identifier (format: `pk_*`) |\n| `name` | string | User/owner name |\n| `model` | string | Model for this key (glm-4.7, glm-4.5-air, etc.) |\n| `token_limit_per_5h` | number | Token quota per 5-hour rolling window |\n| `expiry_date` | string | ISO 8601 timestamp for expiry |\n| `created_at` | string | ISO 8601 creation timestamp |\n| `last_used` | string | ISO 8601 last usage timestamp (auto-updated) |\n| `total_lifetime_tokens` | number | Total all tokens ever used |\n| `usage_windows` | array | Internal tracking array (auto-managed) |\n\n### Example: Create New API Key\n\n```bash\n# Edit file\nnano data/apikeys.json\n\n# Or with jq\njq '.keys += [{\n  \"key\": \"pk_new_user_'\"$(date +%s)\"'\",\n  \"name\": \"New User\",\n  \"model\": \"glm-4.7\",\n  \"token_limit_per_5h\": 50000,\n  \"expiry_date\": \"2026-12-31T23:59:59Z\",\n  \"created_at\": \"'\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"'\",\n  \"last_used\": \"2026-01-18T00:00:00Z\",\n  \"total_lifetime_tokens\": 0,\n  \"usage_windows\": []\n}]' data/apikeys.json > tmp.json && mv tmp.json data/apikeys.json\n```\n\n---\n\n## Rate Limiting\n\n### Rolling 5-Hour Window\n\n- **Window Type**: Rolling window (not fixed reset)\n- **Duration**: 5 hours\n- **Metric**: Total tokens from all requests within active window\n\n### Calculation Example\n\nIf `token_limit_per_5h = 100,000`:\n\n| Time | Tokens | Active Windows (5h) | Total Used | Status |\n|------|--------|---------------------|------------|--------|\n| 00:00 | 10,000 | [(00:00-05:00, 10K)] | 10,000 | OK |\n| 02:00 | 20,000 | [(00:00-05:00, 10K), (02:00-07:00, 20K)] | 30,000 | OK |\n| 04:00 | 50,000 | [(00:00-05:00, 10K), (02:00-07:00, 20K), (04:00-09:00, 50K)] | 80,000 | OK |\n| 04:30 | 30,000 | [(00:00-05:00, 10K), (02:00-07:00, 20K), (04:00-09:00, 50K), (04:30-09:30, 30K)] | 110,000 | **RATE LIMITED** |\n\n### Rate Limited Response\n\n```json\n{\n  \"error\": \"Rate limit exceeded. Please try again later.\"\n}\n```\n\nHTTP Status: `429 Too Many Requests`\n\n---\n\n## Capacity & Scaling\n\n### Single Instance Capacity\n\nWith default setup (Docker, 1 CPU, 512MB RAM):\n- **Concurrent Requests**: ~50-100\n- **Requests/second**: ~100-500 (depending on response size)\n- **Throughput**: Limited by Z.AI rate limit\n\n### Bottlenecks\n\n1. **Z.AI Rate Limit**: Check Z.AI documentation for limits per API key\n2. **Network**: Bandwidth server <-> Z.AI\n3. **CPU/JSON parsing**: For high-throughput scenarios\n\n### Scaling Options\n\n**Horizontal Scaling (Recommended):**\n```bash\n# Multiple instances behind load balancer\ndocker-compose up --scale proxy-gateway=3\n```\n\n**Vertical Scaling:**\n- Increase CPU/RAM in docker-compose.yml\n- Add Redis for distributed rate limiting\n\n---\n\n## Error Codes\n\n| HTTP Code | Error Type | Description |\n|-----------|------------|-------------|\n| 200 | Success | Request successful |\n| 400 | Bad Request | Invalid request body/params |\n| 401 | Unauthorized | Missing/invalid API key |\n| 403 | Forbidden | API key expired |\n| 429 | Rate Limited | Quota exceeded |\n| 500 | Server Error | Internal server error |\n| 502 | Bad Gateway | Upstream (Z.AI) error |\n\n---\n\n## User Information\n\n### Share with Users\n\nProvide the following information to each user:\n\n```\n\ud83d\udcdd API Access Information\n\nEndpoint: http://your-domain.com/v1/chat/completions\nMethod: POST\nHeaders:\n  Authorization: Bearer YOUR_API_KEY\n  Content-Type: application/json\n\nYour API Key: pk_xxxxx\nQuota: 100,000 tokens per 5 hours\nExpiry: 2026-12-31\n\nCheck quota: http://your-domain.com/stats\nDocumentation: http://your-domain.com/docs\n\nExample Request:\ncurl -X POST http://your-domain.com/v1/chat/completions \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"glm-4.7\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}'\n```\n\n### FAQ\n\n**Q: Does it support streaming?**\nA: Yes, set `\"stream\": true` in the request body for streaming response (both OpenAI and Anthropic formats).\n\n**Q: Does it support Anthropic Messages API?**\nA: Yes! Use endpoint `/v1/messages` with Anthropic format. The proxy will auto-forward to Z.AI Anthropic-compatible API.\n\n**Q: Does it support models other than glm-4.7?**\nA: Yes, glm-4.5-air, glm-4.7, glm-4.5-flash, etc. Check Z.AI docs for full list.\n\n**Q: What if quota runs out?**\nA: Wait until the 5-hour window ends, or request admin to increase limit.\n\n**Q: Is my data stored?**\nA: No logging of request/response. Only token usage is tracked.\n\n**Q: What's the difference between OpenAI-compatible vs Anthropic-compatible?**\nA: OpenAI-compatible (`/v1/chat/completions`) uses OpenAI format. Anthropic-compatible (`/v1/messages`) uses Anthropic Messages API format. Both are proxied to Z.AI glm-4.7.\n\n---\n\n## Troubleshooting\n\n### Cache Issues\n\n**Cache hit rate is low (<95%)**\n```bash\n# Check cache statistics\ncurl -H \"Authorization: Bearer pk_your_key\" http://localhost:3030/cache-stats\n\n# Enable warm-up to pre-load all keys on startup\nCACHE_WARMUP_ON_START=true\n```\n\n**API key changes not reflected**\n```bash\n# Cache has 5-minute TTL. Wait for expiration or restart service:\ndocker-compose restart\n\n# Or disable cache temporarily for testing\nCACHE_ENABLED=false\n```\n\n**Debug cache behavior**\n```bash\n# Enable debug logging to see cache operations\nCACHE_LOG_LEVEL=debug\n\n# Check logs for cache hits/misses\ndocker-compose logs -f | grep \"\\[cache\\]\"\n```\n\n**Cache using too much memory**\n```bash\n# Reduce maximum cache size\nCACHE_MAX_SIZE=500\n\n# Check current cache size\ncurl -H \"Authorization: Bearer pk_your_key\" http://localhost:3030/cache-stats\n```\n\n**Disable cache entirely**\n```bash\n# Set environment variable\nCACHE_ENABLED=false\n\n# Then restart service\ndocker-compose restart\n```\n\n### Container won't start\n```bash\n# Check logs\ndocker-compose logs -f\n\n# Rebuild\ndocker-compose up --build -d\n```\n\n### Port conflict\n```bash\n# Change PORT in .env\nPORT=3031\n\n# Or kill process using port\nlsof -ti:3030 | xargs kill -9\n```\n\n### API key invalid/expired\n```bash\n# Check apikeys.json\ncat data/apikeys.json | jq .\n\n# Update expiry date\njq '.keys[0].expiry_date = \"2027-12-31T23:59:59Z\"' data/apikeys.json > tmp.json && mv tmp.json data/apikeys.json\n```\n\n### Z.AI error\n```bash\n# Check Z.AI_API_KEY is valid\ncurl -H \"Authorization: Bearer YOUR_ZAI_KEY\" https://api.z.ai/api/coding/paas/v4/models\n\n# Check Z.AI rate limit\n# (Need to check in Z.AI dashboard)\n```\n\n---\n\n## Development\n\n### Run tests\n```bash\nbun test\n```\n\n### Build\n```bash\nbun build src/index.ts --outdir /tmp/build\n```\n\n### Type check\n```bash\nbun run typecheck\n```\n\n### Lint\n```bash\nbun run lint\n```\n\n---\n\n## Available Models\n\n| Model | Description | Context | Max Output |\n|-------|-------------|---------|------------|\n| glm-4.7 | High-intelligence flagship | 200K | 96K |\n| glm-4.5-air | High cost-performance | 128K | 96K |\n| glm-4.5-flash | Free model | 128K | 96K |\n\n---\n\n## License\n\nMIT\n\n---\n\n## Support\n\nFor issues and questions, please contact [ajianaz](https://github.com/ajianaz).\n",
        "last_modified": "2026-01-22T13:20:55.576797"
      },
      "task_intent": {
        "title": "Implement in-memory API key cache with TTL to eliminate file I/O on every request",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-22T12:46:07.610661",
  "last_updated": "2026-01-22T13:20:55.409770"
}