{
  "file_path": "test/benchmarks/cache-benchmark.test.ts",
  "main_branch_history": [],
  "task_views": {
    "006-implement-in-memory-api-key-cache-with-ttl-to-elim": {
      "task_id": "006-implement-in-memory-api-key-cache-with-ttl-to-elim",
      "branch_point": {
        "commit_hash": "e4ccb2c239067a08687940247e7dc3c37228e546",
        "content": "",
        "timestamp": "2026-01-22T13:20:55.122162"
      },
      "worktree_state": {
        "content": "/**\n * Performance benchmarks for API key cache implementation\n *\n * These benchmarks measure the performance improvement from using the in-memory\n * LRU cache versus file-based lookups. Results demonstrate:\n * - Latency reduction for cache hits vs misses\n * - Throughput improvement under concurrent load\n * - I/O reduction percentage\n *\n * Run with: bun test test/benchmarks/cache-benchmark.test.ts\n */\n\nimport { describe, it, expect, beforeEach, beforeAll, afterAll } from 'vitest';\nimport { LRUCacheImpl } from '../../src/cache.js';\nimport { findApiKey, readApiKeys, writeApiKeys } from '../../src/storage.js';\nimport type { ApiKey, ApiKeysData } from '../../src/types.js';\nimport fs from 'fs';\nimport path from 'path';\n\n// Test data file path (separate from production data)\nconst TEST_DATA_FILE = path.join(process.cwd(), 'data/test-apikeys-benchmark.json');\n\n// Helper function to create test API key data\nfunction createTestApiKeys(count: number): ApiKey[] {\n  const keys: ApiKey[] = [];\n  const now = new Date().toISOString();\n\n  for (let i = 0; i < count; i++) {\n    keys.push({\n      key: `pk_test_benchmark_${i}`,\n      name: `Test Key ${i}`,\n      model: 'glm-4',\n      token_limit_per_5h: 1000000,\n      expiry_date: new Date(Date.now() + 365 * 24 * 60 * 60 * 1000).toISOString(),\n      created_at: now,\n      last_used: now,\n      total_lifetime_tokens: 0,\n      usage_windows: [],\n    });\n  }\n\n  return keys;\n}\n\n// Setup test data file\nasync function setupTestData(count: number): Promise<void> {\n  const dataDir = path.dirname(TEST_DATA_FILE);\n  if (!fs.existsSync(dataDir)) {\n    fs.mkdirSync(dataDir, { recursive: true });\n  }\n\n  const testData: ApiKeysData = {\n    keys: createTestApiKeys(count),\n  };\n\n  await fs.promises.writeFile(TEST_DATA_FILE, JSON.stringify(testData, null, 2), 'utf-8');\n}\n\n// Cleanup test data file\nasync function cleanupTestData(): Promise<void> {\n  try {\n    await fs.promises.unlink(TEST_DATA_FILE);\n  } catch {\n    // Ignore if file doesn't exist\n  }\n}\n\n// Helper to run a benchmark and return statistics\nfunction runBenchmark(\n  name: string,\n  fn: () => void | Promise<void>,\n  iterations: number = 1000\n): { name: string; iterations: number; totalTime: number; avgTime: number; opsPerSec: number } {\n  const start = performance.now();\n\n  for (let i = 0; i < iterations; i++) {\n    fn();\n  }\n\n  const end = performance.now();\n  const totalTime = end - start;\n  const avgTime = totalTime / iterations;\n  const opsPerSec = (iterations / totalTime) * 1000;\n\n  return {\n    name,\n    iterations,\n    totalTime,\n    avgTime,\n    opsPerSec,\n  };\n}\n\n// Async version of benchmark helper\nasync function runBenchmarkAsync(\n  name: string,\n  fn: () => Promise<void>,\n  iterations: number = 100\n): Promise<{ name: string; iterations: number; totalTime: number; avgTime: number; opsPerSec: number }> {\n  const start = performance.now();\n\n  for (let i = 0; i < iterations; i++) {\n    await fn();\n  }\n\n  const end = performance.now();\n  const totalTime = end - start;\n  const avgTime = totalTime / iterations;\n  const opsPerSec = (iterations / totalTime) * 1000;\n\n  return {\n    name,\n    iterations,\n    totalTime,\n    avgTime,\n    opsPerSec,\n  };\n}\n\ndescribe('Cache Performance Benchmarks', () => {\n  describe('Basic cache operations', () => {\n    let cache: LRUCacheImpl<string>;\n\n    beforeEach(() => {\n      cache = new LRUCacheImpl<string>(1000, 5000);\n    });\n\n    it('should measure cache set operation performance', () => {\n      const result = runBenchmark('cache set', () => {\n        cache.set(`key_${Math.random()}`, 'value');\n      }, 10000);\n\n      expect(result.avgTime).toBeLessThan(1); // Target: <1ms per operation\n      expect(result.opsPerSec).toBeGreaterThan(1000);\n    });\n\n    it('should measure cache get operation (hit) performance', () => {\n      cache.set('test_key', 'test_value');\n\n      const result = runBenchmark('cache get (hit)', () => {\n        cache.get('test_key');\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01); // Target: <0.01ms per operation\n      expect(result.opsPerSec).toBeGreaterThan(100000);\n    });\n\n    it('should measure cache get operation (miss) performance', () => {\n      const result = runBenchmark('cache get (miss)', () => {\n        cache.get('nonexistent_key');\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01); // Target: <0.01ms per operation\n      expect(result.opsPerSec).toBeGreaterThan(100000);\n    });\n\n    it('should measure cache has operation performance', () => {\n      cache.set('test_key', 'test_value');\n\n      const result = runBenchmark('cache has', () => {\n        cache.has('test_key');\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01);\n      expect(result.opsPerSec).toBeGreaterThan(100000);\n    });\n\n    it('should measure cache delete operation performance', () => {\n      const result = runBenchmark('cache delete', () => {\n        cache.set('test_key', 'test_value');\n        cache.delete('test_key');\n      }, 10000);\n\n      expect(result.avgTime).toBeLessThan(1);\n      expect(result.opsPerSec).toBeGreaterThan(1000);\n    });\n  });\n\n  describe('Cache vs file I/O performance', () => {\n    const originalDataFile = process.env.DATA_FILE;\n\n    beforeAll(async () => {\n      process.env.DATA_FILE = TEST_DATA_FILE;\n      process.env.CACHE_ENABLED = 'false';\n      await setupTestData(100);\n    });\n\n    afterAll(async () => {\n      process.env.DATA_FILE = originalDataFile;\n      process.env.CACHE_ENABLED = 'true';\n      await cleanupTestData();\n    });\n\n    it('should measure file read operation (baseline)', async () => {\n      const result = await runBenchmarkAsync('file read', async () => {\n        await readApiKeys();\n      }, 100);\n\n      // File I/O is typically slower than cache, but with small data may be fast\n      expect(result.avgTime).toBeGreaterThan(0.01);\n    });\n\n    it('should measure file read with findApiKey (cache disabled)', async () => {\n      const result = await runBenchmarkAsync('findApiKey without cache', async () => {\n        await findApiKey('pk_test_benchmark_0');\n      }, 100);\n\n      // File I/O with small test data may be fast, but should still be measurable\n      expect(result.avgTime).toBeGreaterThan(0.001);\n    });\n\n    it('should demonstrate cache is >10x faster than file I/O', async () => {\n      // Measure cache hit performance\n      const cacheImpl = new LRUCacheImpl<ApiKey>(1000, 5000);\n      const testKey = createTestApiKeys(1)[0];\n      cacheImpl.set(testKey.key, testKey);\n\n      const cacheResult = runBenchmark('cache hit', () => {\n        cacheImpl.get(testKey.key);\n      }, 10000);\n\n      // Measure file I/O performance\n      const fileResult = await runBenchmarkAsync('file I/O', async () => {\n        await findApiKey('pk_test_benchmark_0');\n      }, 100);\n\n      const speedup = fileResult.avgTime / cacheResult.avgTime;\n      expect(speedup).toBeGreaterThan(10); // Cache should be >10x faster\n    });\n  });\n\n  describe('Cache hit performance improvement', () => {\n    let cache: LRUCacheImpl<ApiKey>;\n    const testKeys: ApiKey[] = [];\n\n    beforeAll(() => {\n      cache = new LRUCacheImpl<ApiKey>(1000, 5000);\n      testKeys.push(...createTestApiKeys(10));\n\n      for (const key of testKeys) {\n        cache.set(key.key, key);\n      }\n    });\n\n    it('should measure cache hit - single key retrieval', () => {\n      const result = runBenchmark('single key retrieval', () => {\n        cache.get('pk_test_benchmark_0');\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01);\n      expect(result.opsPerSec).toBeGreaterThan(100000);\n    });\n\n    it('should measure cache hit - random key retrieval', () => {\n      const result = runBenchmark('random key retrieval', () => {\n        const randomIndex = Math.floor(Math.random() * 10);\n        cache.get(`pk_test_benchmark_${randomIndex}`);\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01);\n    });\n\n    it('should measure cache hit - sequential key retrieval (10 keys)', () => {\n      const result = runBenchmark('sequential retrieval', () => {\n        for (let i = 0; i < 10; i++) {\n          cache.get(`pk_test_benchmark_${i}`);\n        }\n      }, 10000);\n\n      expect(result.avgTime).toBeLessThan(0.1); // 10 operations should still be <0.1ms\n    });\n  });\n\n  describe('Concurrent access performance', () => {\n    let cache: LRUCacheImpl<string>;\n    const testKeys: string[] = [];\n\n    beforeAll(() => {\n      cache = new LRUCacheImpl<string>(1000, 5000);\n      for (let i = 0; i < 100; i++) {\n        const key = `key_${i}`;\n        testKeys.push(key);\n        cache.set(key, `value_${i}`);\n      }\n    });\n\n    it('should handle 100 read operations efficiently', () => {\n      const result = runBenchmark('100 concurrent reads', () => {\n        for (let i = 0; i < 100; i++) {\n          const key = testKeys[i % 10];\n          cache.get(key);\n        }\n      }, 1000);\n\n      expect(result.avgTime).toBeLessThan(1); // 100 reads should be <1ms\n    });\n\n    it('should handle 1000 read operations efficiently', () => {\n      const result = runBenchmark('1000 concurrent reads', () => {\n        for (let i = 0; i < 1000; i++) {\n          const key = testKeys[i % 50];\n          cache.get(key);\n        }\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(10); // 1000 reads should be <10ms\n    });\n\n    it('should handle mixed operations efficiently', () => {\n      const result = runBenchmark('mixed operations', () => {\n        cache.get(testKeys[Math.floor(Math.random() * 100)]);\n        cache.set(`new_key_${Math.random()}`, 'value');\n        cache.delete(testKeys[Math.floor(Math.random() * 10)]);\n      }, 10000);\n\n      expect(result.avgTime).toBeLessThan(1);\n    });\n  });\n\n  describe('LRU eviction performance', () => {\n    it('should measure LRU eviction at capacity', () => {\n      const result = runBenchmark('LRU eviction at capacity', () => {\n        const smallCache = new LRUCacheImpl<string>(100, 5000);\n        for (let i = 0; i < 100; i++) {\n          smallCache.set(`key_${i}`, `value_${i}`);\n        }\n        smallCache.set('key_100', 'value_100'); // Triggers eviction\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(10); // Even with eviction, should be fast\n    });\n\n    it('should handle continuous cache churn', () => {\n      const result = runBenchmark('continuous LRU turnover', () => {\n        const smallCache = new LRUCacheImpl<string>(100, 5000);\n        for (let i = 0; i < 1000; i++) {\n          smallCache.set(`key_${i}`, `value_${i}`);\n        }\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(50);\n    });\n  });\n\n  describe('TTL expiration performance', () => {\n    it('should measure TTL check for valid entry', () => {\n      const cache = new LRUCacheImpl<string>(1000, 5000);\n      cache.set('test_key', 'test_value', 5000);\n\n      const result = runBenchmark('TTL check (valid)', () => {\n        cache.get('test_key');\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01);\n    });\n\n    it('should measure expired entry removal', async () => {\n      const result = await runBenchmarkAsync('TTL expiration', async () => {\n        const cache = new LRUCacheImpl<string>(1000, 1);\n        cache.set('test_key', 'test_value', 1);\n        await new Promise(resolve => setTimeout(resolve, 10));\n        cache.get('test_key');\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(20); // Even with expiration, should be fast\n    });\n  });\n\n  describe('Statistics tracking overhead', () => {\n    let cache: LRUCacheImpl<string>;\n\n    beforeEach(() => {\n      cache = new LRUCacheImpl<string>(1000, 5000);\n    });\n\n    it('should measure getStats operation overhead', () => {\n      cache.set('key1', 'value1');\n      cache.get('key1');\n\n      const result = runBenchmark('getStats', () => {\n        cache.getStats();\n      }, 10000);\n\n      expect(result.avgTime).toBeLessThan(0.1);\n    });\n\n    it('should measure resetStats operation overhead', () => {\n      const result = runBenchmark('resetStats', () => {\n        cache.set('key1', 'value1');\n        cache.get('key1');\n        cache.resetStats();\n      }, 10000);\n\n      expect(result.avgTime).toBeLessThan(1);\n    });\n  });\n\n  describe('Real-world API key lookup scenarios', () => {\n    let cache: LRUCacheImpl<ApiKey>;\n    const apiKeys: ApiKey[] = [];\n    const originalDataFile = process.env.DATA_FILE;\n\n    beforeAll(async () => {\n      cache = new LRUCacheImpl<ApiKey>(1000, 300000);\n\n      const now = new Date().toISOString();\n      for (let i = 0; i < 50; i++) {\n        const apiKey: ApiKey = {\n          key: `sk_live_${i}${Math.random().toString(36).substring(2, 15)}`,\n          name: `Production API Key ${i}`,\n          model: i % 3 === 0 ? 'glm-4' : 'glm-4.7',\n          token_limit_per_5h: 1000000,\n          expiry_date: new Date(Date.now() + 365 * 24 * 60 * 60 * 1000).toISOString(),\n          created_at: now,\n          last_used: now,\n          total_lifetime_tokens: Math.floor(Math.random() * 1000000),\n          usage_windows: [],\n        };\n        apiKeys.push(apiKey);\n        cache.set(apiKey.key, apiKey);\n      }\n\n      process.env.DATA_FILE = TEST_DATA_FILE;\n      process.env.CACHE_ENABLED = 'false';\n      await setupTestData(50);\n    });\n\n    afterAll(async () => {\n      process.env.DATA_FILE = originalDataFile;\n      process.env.CACHE_ENABLED = 'true';\n      await cleanupTestData();\n    });\n\n    it('should measure API key lookup cache hit (hot path)', () => {\n      const result = runBenchmark('API key cache hit', () => {\n        const randomKey = apiKeys[Math.floor(Math.random() * 50)];\n        cache.get(randomKey.key);\n      }, 100000);\n\n      expect(result.avgTime).toBeLessThan(0.01); // Target: <0.01ms\n      expect(result.opsPerSec).toBeGreaterThan(100000);\n    });\n\n    it('should measure API key lookup file I/O (cold path)', async () => {\n      const result = await runBenchmarkAsync('API key file I/O', async () => {\n        await findApiKey(`pk_test_benchmark_${Math.floor(Math.random() * 50)}`);\n      }, 100);\n\n      // File I/O should complete successfully (performance varies by system)\n      expect(result.avgTime).toBeGreaterThan(0);\n      expect(result.iterations).toBe(100);\n    });\n\n    it('should simulate typical request pattern (90% cache hit rate)', () => {\n      const result = runBenchmark('typical request pattern', () => {\n        for (let i = 0; i < 100; i++) {\n          if (i < 90) {\n            cache.get(apiKeys[i % 10].key);\n          } else {\n            cache.get(`nonexistent_key_${i}`);\n          }\n        }\n      }, 1000);\n\n      expect(result.avgTime).toBeLessThan(1); // 100 operations should be <1ms\n    });\n  });\n\n  describe('Cache warm-up performance', () => {\n    it('should measure warm-up time for 100 keys', () => {\n      const result = runBenchmark('warm-up 100 keys', () => {\n        const cache = new LRUCacheImpl<ApiKey>(1000, 300000);\n        const keys = createTestApiKeys(100);\n        for (const key of keys) {\n          cache.set(key.key, key);\n        }\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(50); // Should complete in <50ms\n    });\n\n    it('should measure warm-up time for 500 keys', () => {\n      const result = runBenchmark('warm-up 500 keys', () => {\n        const cache = new LRUCacheImpl<ApiKey>(1000, 300000);\n        const keys = createTestApiKeys(500);\n        for (const key of keys) {\n          cache.set(key.key, key);\n        }\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(200); // Should complete in <200ms\n    });\n\n    it('should measure warm-up time for 1000 keys (max size)', () => {\n      const result = runBenchmark('warm-up 1000 keys', () => {\n        const cache = new LRUCacheImpl<ApiKey>(1000, 300000);\n        const keys = createTestApiKeys(1000);\n        for (const key of keys) {\n          cache.set(key.key, key);\n        }\n      }, 10);\n\n      expect(result.avgTime).toBeLessThan(500); // Should complete in <500ms\n    });\n  });\n\n  describe('Memory efficiency', () => {\n    it('should handle memory usage for 100 entries', () => {\n      const result = runBenchmark('memory 100 entries', () => {\n        const cache = new LRUCacheImpl<string>(100, 5000);\n        for (let i = 0; i < 100; i++) {\n          cache.set(`key_${i}`, `value_${i}`.repeat(100));\n        }\n      }, 100);\n\n      expect(result.avgTime).toBeLessThan(50);\n    });\n\n    it('should handle memory usage for 1000 entries', () => {\n      const result = runBenchmark('memory 1000 entries', () => {\n        const cache = new LRUCacheImpl<string>(1000, 5000);\n        for (let i = 0; i < 1000; i++) {\n          cache.set(`key_${i}`, `value_${i}`.repeat(100));\n        }\n      }, 10);\n\n      expect(result.avgTime).toBeLessThan(500);\n    });\n  });\n\n  describe('Performance targets verification', () => {\n    it('should meet all acceptance criteria for cache performance', async () => {\n      const cache = new LRUCacheImpl<ApiKey>(1000, 300000);\n      const testKey = createTestApiKeys(1)[0];\n      cache.set(testKey.key, testKey);\n\n      // Measure cache hit latency\n      const cacheStart = performance.now();\n      for (let i = 0; i < 10000; i++) {\n        cache.get(testKey.key);\n      }\n      const cacheEnd = performance.now();\n      const avgCacheLatency = (cacheEnd - cacheStart) / 10000;\n\n      // Verify cache hit latency < 1ms\n      expect(avgCacheLatency).toBeLessThan(1);\n\n      // Measure throughput (operations per second)\n      const opsPerSec = 1000 / avgCacheLatency;\n      expect(opsPerSec).toBeGreaterThan(1000);\n\n      // Verify cache vs file I/O speedup\n      process.env.DATA_FILE = TEST_DATA_FILE;\n      process.env.CACHE_ENABLED = 'false';\n      await setupTestData(1);\n\n      const fileStart = performance.now();\n      for (let i = 0; i < 100; i++) {\n        await findApiKey('pk_test_benchmark_0');\n      }\n      const fileEnd = performance.now();\n      const avgFileLatency = (fileEnd - fileStart) / 100;\n\n      const speedup = avgFileLatency / avgCacheLatency;\n      // With small test data, speedup may vary, but cache should still be faster\n      expect(speedup).toBeGreaterThan(2); // At least 2x improvement\n\n      process.env.DATA_FILE = process.env.DATA_FILE || '';\n      process.env.CACHE_ENABLED = 'true';\n      await cleanupTestData();\n    });\n  });\n});\n\n/**\n * Performance Results Summary\n *\n * Expected results based on acceptance criteria:\n *\n * 1. Latency Reduction:\n *    - Cache hit: <1ms (target)\n *    - File I/O: 5-50ms (baseline)\n *    - Improvement: 10-50x faster\n *\n * 2. Throughput:\n *    - Cache hit path: >10,000 ops/sec\n *    - File I/O path: 200-1,000 ops/sec\n *    - Improvement: 10-50x higher throughput\n *\n * 3. I/O Reduction:\n *    - With 95%+ cache hit rate: >95% reduction in file reads\n *    - Concurrent load: Eliminates file locking contention\n *\n * 4. LRU Eviction:\n *    - O(1) eviction time\n *    - No performance degradation at max capacity\n *\n * 5. TTL Expiration:\n *    - Lazy expiration check: O(1) on get operation\n *    - Minimal overhead on cache hit path\n *\n * To run benchmarks and see actual results:\n * bun test test/benchmarks/cache-benchmark.test.ts\n *\n * The benchmarks use manual performance measurements with performance.now()\n * to measure actual execution time. Each test includes assertions to verify\n * that performance meets the acceptance criteria.\n */\n",
        "last_modified": "2026-01-22T13:20:55.581450"
      },
      "task_intent": {
        "title": "Implement in-memory API key cache with TTL to eliminate file I/O on every request",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-22T12:46:07.669161",
  "last_updated": "2026-01-22T13:20:55.473171"
}